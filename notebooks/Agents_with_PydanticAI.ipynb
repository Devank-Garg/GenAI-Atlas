{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Pydantic AI for Creating Simple Agents  \n",
    "\n",
    "[PydanticAI](https://github.com/pydantic/pydantic-ai) is a **Python agent framework** designed to simplify the process of building **production-grade applications** with Generative AI.  \n",
    "\n",
    "## Key Features  \n",
    "\n",
    "- ðŸš€ **Type-Safe**: Ensures powerful and informative type checking for better code reliability.  \n",
    "- ðŸ **Python-Centric Design**: Uses familiar Python control flow and agent composition, making it easy to apply standard best practices.  \n",
    "- ðŸ“ **Structured Responses**: Leverages **Pydantic** for validating and structuring model outputs, ensuring consistency across runs.  \n",
    "\n",
    "With **PydanticAI**, you can efficiently build AI-driven applications while maintaining **robust type safety, structured outputs, and Pythonic simplicity**.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 install pydantic AI using  pip install pydantic-ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydantic-ai\n",
      "  Downloading pydantic_ai-0.0.26-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pydantic-ai-slim==0.0.26 (from pydantic-ai-slim[anthropic,cohere,groq,mistral,openai,vertexai]==0.0.26->pydantic-ai)\n",
      "  Downloading pydantic_ai_slim-0.0.26-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting eval-type-backport>=0.2.0 (from pydantic-ai-slim==0.0.26->pydantic-ai-slim[anthropic,cohere,groq,mistral,openai,vertexai]==0.0.26->pydantic-ai)\n",
      "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting griffe>=1.3.2 (from pydantic-ai-slim==0.0.26->pydantic-ai-slim[anthropic,cohere,groq,mistral,openai,vertexai]==0.0.26->pydantic-ai)\n",
      "  Downloading griffe-1.5.7-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting httpx>=0.27 (from pydantic-ai-slim==0.0.26->pydantic-ai-slim[anthropic,cohere,groq,mistral,openai,vertexai]==0.0.26->pydantic-ai)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting logfire-api>=1.2.0 (from pydantic-ai-slim==0.0.26->pydantic-ai-slim[anthropic,cohere,groq,mistral,openai,vertexai]==0.0.26->pydantic-ai)\n",
      "  Downloading logfire_api-3.6.4-py3-none-any.whl.metadata (971 bytes)\n",
      "Collecting pydantic-graph==0.0.26 (from pydantic-ai-slim==0.0.26->pydantic-ai-slim[anthropic,cohere,groq,mistral,openai,vertexai]==0.0.26->pydantic-ai)\n",
      "  Downloading pydantic_graph-0.0.26-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting pydantic>=2.10 (from pydantic-ai-slim==0.0.26->pydantic-ai-slim[anthropic,cohere,groq,mistral,openai,vertexai]==0.0.26->pydantic-ai)\n",
      "  Using cached pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting anthropic>=0.40.0 (from pydantic-ai-slim[anthropic,cohere,groq,mistral,openai,vertexai]==0.0.26->pydantic-ai)\n",
      "  Downloading anthropic-0.47.2-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting cohere>=5.13.11 (from pydantic-ai-slim[anthropic,cohere,groq,mistral,openai,vertexai]==0.0.26->pydantic-ai)\n",
      "  Downloading cohere-5.13.12-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting groq>=0.12.0 (from pydantic-ai-slim[anthropic,cohere,groq,mistral,openai,vertexai]==0.0.26->pydantic-ai)\n",
      "  Downloading groq-0.18.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting mistralai>=1.2.5 (from pydantic-ai-slim[anthropic,cohere,groq,mistral,openai,vertexai]==0.0.26->pydantic-ai)\n",
      "  Downloading mistralai-1.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting openai>=1.61.0 (from pydantic-ai-slim[anthropic,cohere,groq,mistral,openai,vertexai]==0.0.26->pydantic-ai)\n",
      "  Downloading openai-1.64.0-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting google-auth>=2.36.0 (from pydantic-ai-slim[anthropic,cohere,groq,mistral,openai,vertexai]==0.0.26->pydantic-ai)\n",
      "  Using cached google_auth-2.38.0-py2.py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting requests>=2.32.3 (from pydantic-ai-slim[anthropic,cohere,groq,mistral,openai,vertexai]==0.0.26->pydantic-ai)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from anthropic>=0.40.0->pydantic-ai-slim[anthropic,cohere,groq,mistral,openai,vertexai]==0.0.26->pydantic-ai)\n",
      "  Using cached anyio-4.8.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting distro<2,>=1.7.0 (from anthropic>=0.40.0->pydantic-ai-slim[anthropic,cohere,groq,mistral,openai,vertexai]==0.0.26->pydantic-ai)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from anthropic>=0.40.0->pydantic-ai-slim[anthropic,cohere,groq,mistral,openai,vertexai]==0.0.26->pydantic-ai)\n",
      "  Using cached jiter-0.8.2-cp312-cp312-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting sniffio (from anthropic>=0.40.0->pydantic-ai-slim[anthropic,cohere,groq,mistral,openai,vertexai]==0.0.26->pydantic-ai)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting typing-extensions<5,>=4.10 (from anthropic>=0.40.0->pydantic-ai-slim[anthropic,cohere,groq,mistral,openai,vertexai]==0.0.26->pydantic-ai)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting fastavro<2.0.0,>=1.9.4 (from cohere>=5.13.11->pydantic-ai-slim[anthropic,cohere,groq,mistral,openai,vertexai]==0.0.26->pydantic-ai)\n",
      "  Downloading fastavro-1.10.0-cp312-cp312-win_amd64.whl.metadata (5.7 kB)\n",
      "Collecting httpx-sse==0.4.0 (from cohere>=5.13.11->pydantic-ai-slim[anthropic,cohere,groq,mistral,openai,vertexai]==0.0.26->pydantic-ai)\n",
      "  Using cached httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting pydantic-core<3.0.0,>=2.18.2 (from cohere>=5.13.11->pydantic-ai-slim[anthropic,cohere,groq,mistral,openai,vertexai]==0.0.26->pydantic-ai)\n",
      "  Downloading pydantic_core-2.30.0-cp312-cp312-win_amd64.whl.metadata (6.7 kB)\n",
      "Collecting tokenizers<1,>=0.15 (from cohere>=5.13.11->pydantic-ai-slim[anthropic,cohere,groq,mistral,openai,vertexai]==0.0.26->pydantic-ai)\n",
      "  Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting types-requests<3.0.0,>=2.0.0 (from cohere>=5.13.11->pydantic-ai-slim[anthropic,cohere,groq,mistral,openai,vertexai]==0.0.26->pydantic-ai)\n",
      "  Downloading types_requests-2.32.0.20241016-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=2.36.0->pydantic-ai-slim[anthropic,cohere,groq,mistral,openai,vertexai]==0.0.26->pydantic-ai)\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=2.36.0->pydantic-ai-slim[anthropic,cohere,groq,mistral,openai,vertexai]==0.0.26->pydantic-ai)\n",
      "  Using cached pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=2.36.0->pydantic-ai-slim[anthropic,cohere,groq,mistral,openai,vertexai]==0.0.26->pydantic-ai)\n",
      "  Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: colorama>=0.4 in c:\\genai 2025\\agents\\lib\\site-packages (from griffe>=1.3.2->pydantic-ai-slim==0.0.26->pydantic-ai-slim[anthropic,cohere,groq,mistral,openai,vertexai]==0.0.26->pydantic-ai) (0.4.6)\n",
      "Collecting certifi (from httpx>=0.27->pydantic-ai-slim==0.0.26->pydantic-ai-slim[anthropic,cohere,groq,mistral,openai,vertexai]==0.0.26->pydantic-ai)\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting httpcore==1.* (from httpx>=0.27->pydantic-ai-slim==0.0.26->pydantic-ai-slim[anthropic,cohere,groq,mistral,openai,vertexai]==0.0.26->pydantic-ai)\n",
      "  Using cached httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting idna (from httpx>=0.27->pydantic-ai-slim==0.0.26->pydantic-ai-slim[anthropic,cohere,groq,mistral,openai,vertexai]==0.0.26->pydantic-ai)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.27->pydantic-ai-slim==0.0.26->pydantic-ai-slim[anthropic,cohere,groq,mistral,openai,vertexai]==0.0.26->pydantic-ai)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting jsonpath-python>=1.0.6 (from mistralai>=1.2.5->pydantic-ai-slim[anthropic,cohere,groq,mistral,openai,vertexai]==0.0.26->pydantic-ai)\n",
      "  Downloading jsonpath_python-1.0.6-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\genai 2025\\agents\\lib\\site-packages (from mistralai>=1.2.5->pydantic-ai-slim[anthropic,cohere,groq,mistral,openai,vertexai]==0.0.26->pydantic-ai) (2.9.0.post0)\n",
      "Collecting typing-inspect>=0.9.0 (from mistralai>=1.2.5->pydantic-ai-slim[anthropic,cohere,groq,mistral,openai,vertexai]==0.0.26->pydantic-ai)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting tqdm>4 (from openai>=1.61.0->pydantic-ai-slim[anthropic,cohere,groq,mistral,openai,vertexai]==0.0.26->pydantic-ai)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.10->pydantic-ai-slim==0.0.26->pydantic-ai-slim[anthropic,cohere,groq,mistral,openai,vertexai]==0.0.26->pydantic-ai)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core<3.0.0,>=2.18.2 (from cohere>=5.13.11->pydantic-ai-slim[anthropic,cohere,groq,mistral,openai,vertexai]==0.0.26->pydantic-ai)\n",
      "  Using cached pydantic_core-2.27.2-cp312-cp312-win_amd64.whl.metadata (6.7 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.32.3->pydantic-ai-slim[anthropic,cohere,groq,mistral,openai,vertexai]==0.0.26->pydantic-ai)\n",
      "  Using cached charset_normalizer-3.4.1-cp312-cp312-win_amd64.whl.metadata (36 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.32.3->pydantic-ai-slim[anthropic,cohere,groq,mistral,openai,vertexai]==0.0.26->pydantic-ai)\n",
      "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth>=2.36.0->pydantic-ai-slim[anthropic,cohere,groq,mistral,openai,vertexai]==0.0.26->pydantic-ai)\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\genai 2025\\agents\\lib\\site-packages (from python-dateutil>=2.8.2->mistralai>=1.2.5->pydantic-ai-slim[anthropic,cohere,groq,mistral,openai,vertexai]==0.0.26->pydantic-ai) (1.17.0)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from tokenizers<1,>=0.15->cohere>=5.13.11->pydantic-ai-slim[anthropic,cohere,groq,mistral,openai,vertexai]==0.0.26->pydantic-ai)\n",
      "  Downloading huggingface_hub-0.29.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.9.0->mistralai>=1.2.5->pydantic-ai-slim[anthropic,cohere,groq,mistral,openai,vertexai]==0.0.26->pydantic-ai)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting filelock (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere>=5.13.11->pydantic-ai-slim[anthropic,cohere,groq,mistral,openai,vertexai]==0.0.26->pydantic-ai)\n",
      "  Using cached filelock-3.17.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere>=5.13.11->pydantic-ai-slim[anthropic,cohere,groq,mistral,openai,vertexai]==0.0.26->pydantic-ai)\n",
      "  Using cached fsspec-2025.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\genai 2025\\agents\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere>=5.13.11->pydantic-ai-slim[anthropic,cohere,groq,mistral,openai,vertexai]==0.0.26->pydantic-ai) (24.2)\n",
      "Collecting pyyaml>=5.1 (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere>=5.13.11->pydantic-ai-slim[anthropic,cohere,groq,mistral,openai,vertexai]==0.0.26->pydantic-ai)\n",
      "  Using cached PyYAML-6.0.2-cp312-cp312-win_amd64.whl.metadata (2.1 kB)\n",
      "Downloading pydantic_ai-0.0.26-py3-none-any.whl (9.8 kB)\n",
      "Downloading pydantic_ai_slim-0.0.26-py3-none-any.whl (105 kB)\n",
      "Downloading pydantic_graph-0.0.26-py3-none-any.whl (19 kB)\n",
      "Downloading anthropic-0.47.2-py3-none-any.whl (239 kB)\n",
      "Downloading cohere-5.13.12-py3-none-any.whl (252 kB)\n",
      "Using cached httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
      "Using cached google_auth-2.38.0-py2.py3-none-any.whl (210 kB)\n",
      "Downloading griffe-1.5.7-py3-none-any.whl (128 kB)\n",
      "Downloading groq-0.18.0-py3-none-any.whl (121 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Downloading logfire_api-3.6.4-py3-none-any.whl (75 kB)\n",
      "Downloading mistralai-1.5.0-py3-none-any.whl (271 kB)\n",
      "Downloading openai-1.64.0-py3-none-any.whl (472 kB)\n",
      "Using cached pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "Using cached pydantic_core-2.27.2-cp312-cp312-win_amd64.whl (2.0 MB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached anyio-4.8.0-py3-none-any.whl (96 kB)\n",
      "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Using cached charset_normalizer-3.4.1-cp312-cp312-win_amd64.whl (102 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading fastavro-1.10.0-cp312-cp312-win_amd64.whl (487 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached jiter-0.8.2-cp312-cp312-win_amd64.whl (204 kB)\n",
      "Downloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
      "Using cached pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
      "Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading types_requests-2.32.0.20241016-py3-none-any.whl (15 kB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Downloading huggingface_hub-0.29.1-py3-none-any.whl (468 kB)\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Using cached fsspec-2025.2.0-py3-none-any.whl (184 kB)\n",
      "Using cached PyYAML-6.0.2-cp312-cp312-win_amd64.whl (156 kB)\n",
      "Using cached filelock-3.17.0-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: urllib3, typing-extensions, tqdm, sniffio, pyyaml, pyasn1, mypy-extensions, logfire-api, jsonpath-python, jiter, idna, httpx-sse, h11, griffe, fsspec, filelock, fastavro, eval-type-backport, distro, charset-normalizer, certifi, cachetools, annotated-types, typing-inspect, types-requests, rsa, requests, pydantic-core, pyasn1-modules, httpcore, anyio, pydantic, huggingface-hub, httpx, google-auth, tokenizers, pydantic-graph, openai, mistralai, groq, anthropic, pydantic-ai-slim, cohere, pydantic-ai\n",
      "Successfully installed annotated-types-0.7.0 anthropic-0.47.2 anyio-4.8.0 cachetools-5.5.2 certifi-2025.1.31 charset-normalizer-3.4.1 cohere-5.13.12 distro-1.9.0 eval-type-backport-0.2.2 fastavro-1.10.0 filelock-3.17.0 fsspec-2025.2.0 google-auth-2.38.0 griffe-1.5.7 groq-0.18.0 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 httpx-sse-0.4.0 huggingface-hub-0.29.1 idna-3.10 jiter-0.8.2 jsonpath-python-1.0.6 logfire-api-3.6.4 mistralai-1.5.0 mypy-extensions-1.0.0 openai-1.64.0 pyasn1-0.6.1 pyasn1-modules-0.4.1 pydantic-2.10.6 pydantic-ai-0.0.26 pydantic-ai-slim-0.0.26 pydantic-core-2.27.2 pydantic-graph-0.0.26 pyyaml-6.0.2 requests-2.32.3 rsa-4.9 sniffio-1.3.1 tokenizers-0.21.0 tqdm-4.67.1 types-requests-2.32.0.20241016 typing-extensions-4.12.2 typing-inspect-0.9.0 urllib3-2.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pydantic-ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a simple hello world of AI agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai.models.openai import OpenAIModel\n",
    "from pydantic_ai.agent import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of India is New Delhi. New Delhi has been the seat of power for India since its independence in 1947 and serves as a hub for Indian politics, culture, and international diplomacy.\n",
      "<bound method AgentRunResult.all_messages of AgentRunResult(data='The capital of India is New Delhi. New Delhi has been the seat of power for India since its independence in 1947 and serves as a hub for Indian politics, culture, and international diplomacy.')>\n",
      "AgentRunResult(data='The capital of India is New Delhi. New Delhi has been the seat of power for India since its independence in 1947 and serves as a hub for Indian politics, culture, and international diplomacy.')\n"
     ]
    }
   ],
   "source": [
    "# It is interesting to note that there is no official class for Ollama model integration rather we have to use the OpenAIModel class for Ollama as well ðŸ˜‚\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "Model_ID = 'llama3.2'  # Specify the model u want to run\n",
    "\n",
    "Base_url = 'Http://localhost:11434/v1' # this is the default server url when running ollama\n",
    "\n",
    "ollama_model = OpenAIModel(\n",
    "    model_name= Model_ID,\n",
    "    base_url= Base_url,\n",
    ")\n",
    "\n",
    "\n",
    "agent = Agent(\n",
    "    model= ollama_model,\n",
    "    system_prompt= ['make the reponse more informative by adding 1 line of infromation. ']        # you can change the system prompt to see the difference in result\n",
    ")\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()      # These lines are to handle the event loops of pydantic and jupyter based enviornments ( add these in google colab as well, remove if running in simple .py file)\n",
    "\n",
    "response = agent.run_sync('What is the captial of India ?')\n",
    "\n",
    "print(response.data)\n",
    "print(response.all_messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Outputs with PydanticAI  \n",
    "\n",
    "Now that we have seen how to run a simple agent, let's explore how to ensure **consistent outputs** from LLMs using **structured responses**.  \n",
    "\n",
    "### Why Structured Outputs?  \n",
    "\n",
    "Generative AI models can produce highly variable responses. By leveraging **PydanticAI**, we can:  \n",
    "\n",
    "- âœ… **Enforce Data Consistency**: Define strict output schemas to ensure reliable responses.  \n",
    "- ðŸ— **Improve Parsing & Validation**: Use Pydantic models to automatically validate and structure outputs.  \n",
    "- ðŸš€ **Enhance Usability**: Make AI responses more predictable and easier to work with in production.  \n",
    "\n",
    "Next, weâ€™ll dive into how **PydanticAI** helps achieve structured outputs with minimal effort.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"response\": \"Please provide a detailed description of your experience with the product, allowing us to assist you in writing a constructive review.\",\n",
      "  \"sentiment\": \"neutral\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field  # we will specify our schema in by extending the basemodel class\n",
    "\n",
    "class ResponseType(BaseModel):\n",
    "    \"\"\"Getting structured responses from LLMs along with metadata\"\"\"\n",
    "    response: str  # We want the response type to be str\n",
    "    sentiment: str = Field( description  = 'Customer sentiment analysis') # additional we want model to do\n",
    "\n",
    "\n",
    "agent = Agent(\n",
    "    model= ollama_model,\n",
    "    result_type=ResponseType,\n",
    "    system_prompt= ['You are customer support agent. Help as much as you can .Return results in Structured format only ']        # you can change the system prompt to see the difference in result\n",
    ")\n",
    " \n",
    "\n",
    "response = agent.run_sync(\"This product is very bad,quality is very bad, how to give review of this product\")\n",
    "\n",
    "# print(response)\n",
    "\n",
    "print(response.data.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Step-by-Step Guide: Structured Customer Support Agent with Pydantic and AI Agent\n",
    "\n",
    "## Overview\n",
    "This guide walks through the implementation of a structured AI-powered customer support agent using `pydantic_ai`. The system processes customer queries, extracts structured data, and provides intelligent responses.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Import Required Modules\n",
    "First, import necessary modules for handling structured data and AI processing:\n",
    "\n",
    "```python\n",
    "from pydantic_ai import RunContext\n",
    "from typing import Dict, List, Optional\n",
    "```\n",
    "\n",
    "- `RunContext` is used for managing structured responses.\n",
    "- `List` and `Optional` help define flexible data structures.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2: Define Data Models\n",
    "Define the schemas for `BookOrder` and `CustomerDetails` using `pydantic`.\n",
    "\n",
    "### 2.1 Define the Book Order Schema\n",
    "```python\n",
    "class BookOrder(BaseModel):\n",
    "    \"\"\"Structure for order details.\"\"\"\n",
    "    order_id: str\n",
    "    status: str\n",
    "    items: List[str]\n",
    "```\n",
    "- `order_id`: Unique identifier for the order.\n",
    "- `status`: Current status (e.g., `shipped`, `pending`).\n",
    "- `items`: List of ordered books.\n",
    "\n",
    "### 2.2 Define the Customer Schema\n",
    "```python\n",
    "class CustomerDetails(BaseModel):\n",
    "    \"\"\"Structure for incoming customer queries.\"\"\"\n",
    "    customer_id: str\n",
    "    name: str\n",
    "    email: str\n",
    "    orders: Optional[List[BookOrder]] = None\n",
    "```\n",
    "- `customer_id`: Unique customer identifier.\n",
    "- `name`: Customerâ€™s full name.\n",
    "- `email`: Contact email.\n",
    "- `orders`: A list of book orders (optional).\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3: Create AI Agent with Dependencies\n",
    "Define an AI agent using `pydantic_ai` and configure its response handling.\n",
    "\n",
    "```python\n",
    "agent5 = Agent(\n",
    "    model=ollama_model,  # Define the AI model being used\n",
    "    result_type=ResponseType,  # Expected response structure\n",
    "    deps_type=CustomerDetails,  # The agent processes customer details\n",
    "    retries=3,  # Number of retry attempts for failed queries\n",
    "    system_prompt=(\n",
    "        \"You are an intelligent customer support agent. \"\n",
    "        \"Analyze queries carefully and provide structured responses. \"\n",
    "        \"Always greet the customer and provide a helpful response.\"\n",
    "    ),\n",
    ")\n",
    "```\n",
    "- Uses a predefined AI model (`ollama_model`).\n",
    "- Accepts structured customer details (`CustomerDetails`).\n",
    "- Retries up to three times for better robustness.\n",
    "- A system prompt guides the AI behavior.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 4: Modify System Prompt Dynamically\n",
    "Update the system prompt dynamically to include customer details.\n",
    "\n",
    "```python\n",
    "@agent5.system_prompt\n",
    "async def add_customer_name(ctx: RunContext[CustomerDetails]) -> str:\n",
    "    return f\"Customer details: {(ctx.deps)}\"\n",
    "```\n",
    "- Enhances AI responses with real-time customer data.\n",
    "- `ctx.deps` injects structured details into the prompt.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 5: Create Customer Data\n",
    "Instantiate a sample customer object with an order:\n",
    "\n",
    "```python\n",
    "customer = CustomerDetails(\n",
    "    customer_id=\"1\",\n",
    "    name=\"John Doe\",\n",
    "    email=\"john.doe@example.com\",\n",
    "    orders=[\n",
    "        BookOrder(order_id=\"12345\", status=\"shipped\", items=[\"Harry Potter and the Prisoner of Azkaban\", \"The Divine Comedy\"]),\n",
    "    ],\n",
    ")\n",
    "```\n",
    "- A `CustomerDetails` object is created with one order.\n",
    "- The order contains a status (`shipped`) and two book items.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 6: Run the Agent with Customer Query\n",
    "Use the agent to process a customerâ€™s query about their orders.\n",
    "\n",
    "```python\n",
    "response = agent5.run_sync(user_prompt=\"What did I order?\", deps=customer)\n",
    "```\n",
    "- The agent processes the structured customer details.\n",
    "- The query \"What did I order?\" triggers the AI to extract order details.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 7: Extract and Display the Response\n",
    "Retrieve and print the structured response from the agent.\n",
    "\n",
    "```python\n",
    "response.all_messages()\n",
    "print(response.data.model_dump_json(indent=2))\n",
    "```\n",
    "- `all_messages()` fetches AI-generated messages.\n",
    "- `model_dump_json(indent=2)` neatly formats the response.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 8: Print Customer and Response Details\n",
    "Display structured details including the customerâ€™s name, email, response, and order status.\n",
    "\n",
    "```python\n",
    "print(\n",
    "    \"Customer Details:\\n\"\n",
    "    f\"Name: {customer.name}\\n\"\n",
    "    f\"Email: {customer.email}\\n\\n\"\n",
    "    \"Response Details:\\n\"\n",
    "    f\"{response.data.response}\\n\\n\"\n",
    "    \"Status:\\n\"\n",
    "    f\"{customer.orders[0].status}\\n\\n\"\n",
    ")\n",
    "```\n",
    "- Displays key customer details.\n",
    "- Extracts the order status dynamically.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "This guide provides a structured approach to:\n",
    "1. **Define schemas** using `pydantic` for structured data.\n",
    "2. **Configure an AI-powered agent** for customer support.\n",
    "3. **Integrate dynamic prompts** with customer details.\n",
    "4. **Process and display structured responses** efficiently.\n",
    "\n",
    "This ensures a seamless customer support experience with intelligent, structured responses.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"response\": \"You ordered Harry Potter and the prisoner of Azakban and The Divine Comedy.\",\n",
      "  \"sentiment\": \"\"\n",
      "}\n",
      "Customer Details:\n",
      "Name: John Doe\n",
      "Email: john.doe@example.com\n",
      "\n",
      "Response Details:\n",
      "You ordered Harry Potter and the prisoner of Azakban and The Divine Comedy.\n",
      "\n",
      "Status:\n",
      "shipped\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pydantic_ai import RunContext\n",
    "from pydantic import BaseModel\n",
    "from typing import Dict, List, Optional\n",
    "#  Define order schema\n",
    "\n",
    "\n",
    "class BookOrder(BaseModel):\n",
    "    \"\"\"Structure for order details.\"\"\"\n",
    "\n",
    "    order_id: str\n",
    "    status: str\n",
    "    items: List[str]\n",
    "\n",
    "\n",
    "# Define customer schema\n",
    "class CustomerDetails(BaseModel):\n",
    "    \"\"\"Structure for incoming customer queries.\"\"\"\n",
    "\n",
    "    customer_id: str\n",
    "    name: str\n",
    "    email: str\n",
    "    orders: Optional[List[BookOrder]] = None\n",
    "\n",
    "\n",
    "# Agent with structured output and dependencies\n",
    "agent5 = Agent(\n",
    "    model=ollama_model,\n",
    "    result_type=ResponseType,\n",
    "    deps_type=CustomerDetails,\n",
    "    retries=3,\n",
    "    system_prompt=(\n",
    "        \"You are an intelligent customer support agent. \"\n",
    "        \"Analyze queries carefully and provide structured responses. \"\n",
    "        \"Always great the customer and provide a helpful response.\"\n",
    "    ),  # These are known when writing the code\n",
    ")\n",
    "\n",
    "\n",
    "# Add dynamic system prompt based on dependencies\n",
    "@agent5.system_prompt\n",
    "async def add_customer_name(ctx: RunContext[CustomerDetails]) -> str:\n",
    "    return f\"Customer details: {(ctx.deps)}\"  # These depend in some way on context that isn't known until runtime\n",
    "\n",
    "\n",
    "customer = CustomerDetails(\n",
    "    customer_id=\"1\",\n",
    "    name=\"John Doe\",\n",
    "    email=\"john.doe@example.com\",\n",
    "    orders=[\n",
    "        BookOrder(order_id=\"12345\", status=\"shipped\", items=[\"Harry Potter and the prisoner of Azakban\", \"The Divine Comedy\"]),\n",
    "    ],\n",
    ")\n",
    "\n",
    "response = agent5.run_sync(user_prompt=\"What did I order?\", deps=customer)\n",
    "\n",
    "response.all_messages()\n",
    "print(response.data.model_dump_json(indent=2))\n",
    "\n",
    "print(\n",
    "    \"Customer Details:\\n\"\n",
    "    f\"Name: {customer.name}\\n\"\n",
    "    f\"Email: {customer.email}\\n\\n\"\n",
    "    \"Response Details:\\n\"\n",
    "    f\"{response.data.response}\\n\\n\"\n",
    "    \"Status:\\n\"\n",
    "    f\"{customer.orders[0].status}\\n\\n\"\n",
    "    \n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Let's see to use Tools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mysql-connector-python\n",
      "  Downloading mysql_connector_python-9.2.0-cp312-cp312-win_amd64.whl.metadata (6.2 kB)\n",
      "Downloading mysql_connector_python-9.2.0-cp312-cp312-win_amd64.whl (16.1 MB)\n",
      "   ---------------------------------------- 0.0/16.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.8/16.1 MB 8.5 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 1.0/16.1 MB 3.1 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 2.6/16.1 MB 4.3 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 3.4/16.1 MB 4.2 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 4.5/16.1 MB 4.1 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 5.2/16.1 MB 4.1 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 5.8/16.1 MB 4.1 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 6.8/16.1 MB 4.0 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 7.6/16.1 MB 4.0 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 8.4/16.1 MB 4.0 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 9.2/16.1 MB 4.0 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 10.0/16.1 MB 4.0 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 10.5/16.1 MB 3.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 11.3/16.1 MB 3.9 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 12.1/16.1 MB 3.9 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 12.8/16.1 MB 3.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 13.6/16.1 MB 3.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 14.4/16.1 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 15.2/16.1 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  16.0/16.1 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 16.1/16.1 MB 3.8 MB/s eta 0:00:00\n",
      "Installing collected packages: mysql-connector-python\n",
      "Successfully installed mysql-connector-python-9.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install mysql-connector-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order Details:\n",
      "Username: alice, Shipping Status: Shipped, Shipped Date: 2024-12-01, Delivery Status: Delivered\n",
      "Order Details:\n",
      "Username: alice, Shipping Status: Shipped, Shipped Date: 2024-12-01, Delivery Status: Delivered\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "\n",
    "def create_database():\n",
    "    conn = mysql.connector.connect(host=\"localhost\", user=\"root\", password=\"root\")\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"CREATE DATABASE IF NOT EXISTS shop_db\")\n",
    "    conn.close()\n",
    "\n",
    "def create_table():\n",
    "    conn = mysql.connector.connect(host=\"localhost\", user=\"root\", password=\"root\", database=\"shop_db\")\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS orders\")  # Drop table if it exists\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE orders (\n",
    "            username VARCHAR(50),\n",
    "            shipping_status VARCHAR(100),\n",
    "            shipped_date DATE,\n",
    "            delivery_status VARCHAR(100)\n",
    "        )\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "def insert_sample_data():\n",
    "    conn = mysql.connector.connect(host=\"localhost\", user=\"root\", password=\"root\", database=\"shop_db\")\n",
    "    cursor = conn.cursor()\n",
    "    sample_data = [\n",
    "        (\"alice\", \"Shipped\", \"2024-12-01\", \"Delivered\"),\n",
    "        (\"bob\", \"Out for delivery\", \"2024-12-02\", \"In transit\"),\n",
    "    \n",
    "        (\"charlie\", None, None, None)\n",
    "    ]\n",
    "    cursor.executemany(\"INSERT INTO orders (username, shipping_status, shipped_date, delivery_status) VALUES (%s, %s, %s, %s)\", sample_data)\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def get_order_details():\n",
    "    while True:\n",
    "        username = input(\"Enter username (or type 'exit' to quit): \").strip()\n",
    "        if username.lower() == \"exit\":\n",
    "            break  # Exit the loop\n",
    "\n",
    "        conn = mysql.connector.connect(host=\"localhost\", user=\"root\", password=\"root\", database=\"shop_db\")\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"SELECT * FROM orders WHERE username = %s\", (username,))\n",
    "        orders = cursor.fetchall()\n",
    "        conn.close()\n",
    "        \n",
    "        if not orders:\n",
    "            print(\"No orders found for this user.\")\n",
    "        else:\n",
    "            print(\"Order Details:\")\n",
    "            for order in orders:\n",
    "                print(f\"Username: {order[0]}, Shipping Status: {order[1] if order[1] else 'N/A'}, Shipped Date: {order[2] if order[2] else 'N/A'}, Delivery Status: {order[3] if order[3] else 'N/A'}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_database()\n",
    "    create_table()\n",
    "    insert_sample_data()\n",
    "    get_order_details()  # Now allows multiple queries and an exit option\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_database()\n",
    "    create_table()\n",
    "    insert_sample_data()\n",
    "    get_order_details()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order Details:\n",
      "Username: alice, Shipping Status: Shipped, Shipped Date: 2024-12-01, Delivery Status: Delivered\n",
      "{\n",
      "  \"response\": \"The status of your last order is \\\"shipped\\\".\",\n",
      "  \"sentiment\": \"neutral\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from pydantic_ai import Tool\n",
    "\n",
    "get_order_details()\n",
    "\n",
    "\n",
    "def get_shipping_info(ctx: RunContext[CustomerDetails]) -> str:\n",
    "    \"\"\"Get the customer's shipping information.\"\"\"\n",
    "    return get_order_details[ctx.deps.orders[0].order_id]\n",
    "\n",
    "\n",
    "# Agent with structured output and dependencies\n",
    "agent5 = Agent(\n",
    "    model=ollama_model,\n",
    "    result_type=ResponseType,\n",
    "    deps_type=CustomerDetails,\n",
    "    retries=3,\n",
    "    system_prompt=(\n",
    "        \"You are an intelligent customer support agent. \"\n",
    "        \"Analyze queries carefully and provide structured responses. \"\n",
    "        \"Use tools to look up relevant information.\"\n",
    "        \"Always great the customer and provide a helpful response.\"\n",
    "    ),  # These are known when writing the code\n",
    "    tools=[Tool(get_shipping_info, takes_ctx=True)],  # Add tool via kwarg\n",
    ")\n",
    "\n",
    "\n",
    "@agent5.system_prompt\n",
    "async def add_customer_name(ctx: RunContext[CustomerDetails]) -> str:\n",
    "    return f\"Customer details: {(ctx.deps)}\"\n",
    "\n",
    "\n",
    "response = agent5.run_sync(\n",
    "    user_prompt=\"What's the status of my last order?\", deps=customer\n",
    ")\n",
    "\n",
    "response.all_messages()\n",
    "print(response.data.model_dump_json(indent=2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
