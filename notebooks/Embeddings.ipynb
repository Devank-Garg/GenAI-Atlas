{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§  From Text to Tensors: A Deep Dive into Tokenization & Embeddings\n",
    "\n",
    "Welcome! This notebook is your guide to understanding two of the most fundamental concepts in Natural Language Processing (NLP): **Tokenization** and **Embeddings**.\n",
    "\n",
    "Before a machine learning model can understand human language, we need to convert raw text into a numerical format it can process. This pipeline involves two key steps:\n",
    "\n",
    "1.  **Tokenization**: The process of breaking down a piece of text into smaller units called **tokens**. These can be words, subwords, or characters.\n",
    "2.  **Embeddings**: The process of converting these tokens into numerical vectors. These vectors are designed to capture the semantic meaning and relationships between tokens.\n",
    "\n",
    "Let's explore how this works in practice.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Tokenization - The Art of Splitting Text\n",
    "\n",
    "Think of tokenization like chopping vegetables before cooking. You take a large, unstructured piece of text and break it into small, manageable pieces (tokens) that the model can digest.\n",
    "\n",
    "### 1.1 - Simple Word Tokenization\n",
    "\n",
    "The most basic approach is to split text by spaces and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"NLP is fascinating, isn't it?\"\n",
    "\n",
    "# Using Python's built-in split()\n",
    "basic_tokens = sentence.split(' ')\n",
    "print(f\"Basic split: {basic_tokens}\")\n",
    "\n",
    "# This is a bit naive. Notice how \"fascinating,\" and \"it?\" still have punctuation.\n",
    "# A better approach uses libraries like NLTK or spaCy.\n",
    "\n",
    "# Install NLTK (if you haven't already)\n",
    "!pip install nltk -q\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download the necessary resource for the tokenizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "nltk_tokens = word_tokenize(sentence)\n",
    "print(f\"NLTK split: {nltk_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem with Word Tokenization**: What happens when the model encounters a word it has never seen before during training (an \"out-of-vocabulary\" or OOV word)? Also, how does it handle variations of a word like \"run\", \"running\", and \"ran\"? They are treated as completely separate tokens.\n",
    "\n",
    "This leads us to a more modern and effective approach: **Subword Tokenization**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Subword Tokenization\n",
    "\n",
    "The core idea of subword tokenization is to break down rare words into smaller, meaningful parts, while keeping common words as single tokens.\n",
    "\n",
    "For example, a word like `\"tokenization\"` might be split into `\"token\"` and `\"##ization\"`.\n",
    "\n",
    "**Advantages**:\n",
    "* **Handles OOV words**: It can represent any new word by combining known subwords.\n",
    "* **Reduces vocabulary size**: The model only needs to store a vocabulary of subwords, which is much smaller than storing every single word in a language.\n",
    "* **Captures morphology**: It understands that words like \"running\" and \"runner\" share a common root, `\"run\"`.\n",
    "\n",
    "Popular algorithms include **Byte-Pair Encoding (BPE)** (used by GPT) and **WordPiece** (used by BERT). Today, we'll use the industry-standard `transformers` library from Hugging Face to see this in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the transformers library\n",
    "!pip install transformers -q\n",
    "\n",
    "from transformers import BertTokenizer, GPT2Tokenizer\n",
    "\n",
    "# Load a pre-trained tokenizer for BERT\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load a pre-trained tokenizer for GPT-2\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how these tokenizers handle a complex sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Subword tokenization is powerfully effective.\"\n",
    "\n",
    "# Tokenize with BERT's WordPiece tokenizer\n",
    "bert_tokens = bert_tokenizer.tokenize(text)\n",
    "print(f\"BERT Tokens: {bert_tokens}\")\n",
    "# Notice how 'powerfully' is split into 'powerful' and '##ly'\n",
    "\n",
    "# Tokenize with GPT-2's BPE tokenizer\n",
    "gpt2_tokens = gpt2_tokenizer.tokenize(text)\n",
    "print(f\"GPT-2 Tokens: {gpt2_tokens}\")\n",
    "# Notice how 'Ä tokenization' starts with a 'Ä ' (space) character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - From Tokens to IDs\n",
    "\n",
    "Models don't understand strings like `\"token\"` or `\"##ization\"`. They need numbers. So, the next step is to convert these tokens into unique integer IDs from the tokenizer's vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The encode() method combines tokenization and conversion to IDs\n",
    "bert_input_ids = bert_tokenizer.encode(text)\n",
    "print(f\"BERT Input IDs: {bert_input_ids}\")\n",
    "\n",
    "# Let's see what these IDs correspond to\n",
    "decoded_bert_tokens = bert_tokenizer.convert_ids_to_tokens(bert_input_ids)\n",
    "print(f\"Decoded BERT Tokens: {decoded_bert_tokens}\")\n",
    "\n",
    "# Notice the special tokens [CLS] and [SEP] that BERT adds automatically.\n",
    "# [CLS] (Classification) is used for sentence-level tasks.\n",
    "# [SEP] (Separator) is used to separate multiple sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Embeddings - Giving Meaning to Numbers\n",
    "\n",
    "Now that we have token IDs, we need to represent them in a way that captures their meaning. Simply using the IDs (e.g., 101, 1143, etc.) isn't enough, as the numbers themselves have no inherent relationship to one another.\n",
    "\n",
    "This is where **embeddings** come in. An embedding is a dense vector of real numbers that represents a token. \n",
    "\n",
    "**The Goal**: Tokens with similar meanings should have similar vectors. For example, the vectors for `\"king\"` and `\"queen\"` should be closer to each other than the vector for `\"car\"`.\n",
    "\n",
    "### 2.1 - Why Not One-Hot Encoding?\n",
    "A simple approach would be to create a huge vector for each token, with a `1` at the index corresponding to its vocabulary ID and `0`s everywhere else. \n",
    "\n",
    "**Problems**:\n",
    "* **Sparsity & High Dimensionality**: If our vocabulary has 30,000 tokens, each vector would have 30,000 dimensions, which is computationally expensive.\n",
    "* **No Semantic Relationship**: The vectors are orthogonal, meaning the model can't learn that `\"cat\"` is more similar to `\"kitten\"` than to `\"airplane\"`.\n",
    "\n",
    "### 2.2 - Contextual Embeddings with Transformers\n",
    "\n",
    "Older models like Word2Vec and GloVe produced **static embeddings**, where each word had a single, fixed vector. \n",
    "\n",
    "Modern Transformer models like BERT and GPT produce **contextual embeddings**. The vector for a word changes depending on the sentence it's in. The embedding for `\"bank\"` will be different in `\"river bank\"` vs. `\"money bank\"`.\n",
    "\n",
    "Let's generate some embeddings using a Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# Sentences with the same word (\"bank\") in different contexts\n",
    "sentence1 = \"I sat by the river bank.\"\n",
    "sentence2 = \"I need to go to the bank to deposit money.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the sentences\n",
    "# We use padding to make both sequences the same length and return PyTorch tensors\n",
    "inputs = tokenizer([sentence1, sentence2], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "print(\"Input IDs:\")\n",
    "print(inputs['input_ids'])\n",
    "print(\"\\nAttention Mask:\") # The attention mask tells the model which tokens are real and which are padding\n",
    "print(inputs['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the embeddings from the model\n",
    "# We don't need to calculate gradients for this, so we use torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# The embeddings are in the 'last_hidden_state' attribute\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "print(f\"Shape of embeddings tensor: {last_hidden_states.shape}\")\n",
    "# Shape is (batch_size, sequence_length, hidden_dim)\n",
    "# batch_size = 2 (we passed two sentences)\n",
    "# sequence_length = 11 (the length of the padded sequences)\n",
    "# hidden_dim = 768 (the size of the embedding vector for BERT-base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's prove that the embeddings for `\"bank\"` are different in each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the index of the token 'bank' in each sentence\n",
    "tokens1 = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "tokens2 = tokenizer.convert_ids_to_tokens(inputs['input_ids'][1])\n",
    "\n",
    "bank_index1 = tokens1.index('bank')\n",
    "bank_index2 = tokens2.index('bank')\n",
    "\n",
    "print(f\"Index of 'bank' in sentence 1: {bank_index1}\")\n",
    "print(f\"Index of 'bank' in sentence 2: {bank_index2}\")\n",
    "\n",
    "# Get the embedding vectors for 'bank' from each sentence\n",
    "bank_embedding1 = last_hidden_states[0, bank_index1]\n",
    "bank_embedding2 = last_hidden_states[1, bank_index2]\n",
    "\n",
    "# Compare the two embeddings using Cosine Similarity\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "similarity = cosine_similarity(bank_embedding1.unsqueeze(0), bank_embedding2.unsqueeze(0))\n",
    "\n",
    "print(f\"\\nFirst 5 values of 'bank' embedding 1: {bank_embedding1[:5]}\")\n",
    "print(f\"First 5 values of 'bank' embedding 2: {bank_embedding2[:5]}\")\n",
    "print(f\"\\nCosine Similarity between the two 'bank' embeddings: {similarity.item():.4f}\")\n",
    "\n",
    "# Let's also check the similarity of 'bank' with 'river' in the first sentence\n",
    "river_index1 = tokens1.index('river')\n",
    "river_embedding1 = last_hidden_states[0, river_index1]\n",
    "river_bank_similarity = cosine_similarity(bank_embedding1.unsqueeze(0), river_embedding1.unsqueeze(0))\n",
    "print(f\"Cosine Similarity between 'river' and 'bank' in sentence 1: {river_bank_similarity.item():.4f}\")\n",
    "\n",
    "# And the similarity of 'bank' with 'money' in the second sentence\n",
    "money_index2 = tokens2.index('money')\n",
    "money_embedding2 = last_hidden_states[1, money_index2]\n",
    "money_bank_similarity = cosine_similarity(bank_embedding2.unsqueeze(0), money_embedding2.unsqueeze(0))\n",
    "print(f\"Cosine Similarity between 'money' and 'bank' in sentence 2: {money_bank_similarity.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the embedding for `\"bank\"` is more similar to `\"river\"` in the first context and more similar to `\"money\"` in the second context. This is the power of contextual embeddings!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Visualizing Embeddings\n",
    "\n",
    "It's hard to imagine a 768-dimensional space. To make this more intuitive, we can use dimensionality reduction techniques like **PCA** to project these vectors down to 2D and plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "# Words we want to visualize\n",
    "words = [\n",
    "    # Animals\n",
    "    'cat', 'dog', 'kitten', 'puppy',\n",
    "    # Royalty\n",
    "    'king', 'queen', 'prince', 'princess',\n",
    "    # Objects\n",
    "    'car', 'truck', 'boat', 'plane'\n",
    "]\n",
    "\n",
    "# We need to get the embeddings for these specific words from BERT's embedding layer\n",
    "# Note: These will be static, context-independent embeddings from the first layer\n",
    "word_ids = tokenizer.convert_tokens_to_ids(words)\n",
    "word_embeddings = model.embeddings.word_embeddings.weight[word_ids, :].detach().numpy()\n",
    "\n",
    "# Use PCA to reduce dimensions from 768 to 2\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(word_embeddings)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 10))\n",
    "for i, word in enumerate(words):\n",
    "    x, y = embeddings_2d[i, :]\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(word, (x, y), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')\n",
    "plt.title('2D PCA Visualization of Word Embeddings')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can clearly see clusters forming! The animals are grouped together, the royalty terms are close, and the vehicles form another cluster. This visualizes the semantic relationships captured by the embeddings.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion & Next Steps\n",
    "\n",
    "Congratulations! You've successfully journeyed from raw text to meaningful numerical representations.\n",
    "\n",
    "**You've learned**:\n",
    "1.  **Tokenization**: How to split text into manageable tokens using word and subword strategies.\n",
    "2.  **Vocabulary & IDs**: How to map tokens to numerical IDs.\n",
    "3.  **Embeddings**: Why embeddings are superior to one-hot encoding for representing meaning.\n",
    "4.  **Contextual Embeddings**: How models like BERT create dynamic representations of words based on their context.\n",
    "\n",
    "These tokenized and embedded vectors are the inputs for virtually all modern NLP tasks, from **text classification** and **question answering** to **text generation**."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Tokenization and Embeddings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}